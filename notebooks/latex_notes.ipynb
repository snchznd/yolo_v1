{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confidence Loss Gradient Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let:\n",
    "* $f_w(x)$ be the YOLO network's output given input $x$ and parameter $w$\n",
    "* $C(\\cdot)$ be the function that extracts the predicted confidence from the network output (i.e. $C(f_w(x))$ is the predicted confidence).\n",
    "* $IoU(f_w(x),y)$ be the function that computes the intersection-over-union between the predicted bounding box $(x,y,w,h)$ from the newtork and the ground truth $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 1: IoU is computed within the torch graph\n",
    "We have:\n",
    "$$L_w = \\left( C\\left(f_w(x)\\right) - IoU\\left(f_w(x), y\\right) \\right)^2$$\n",
    "\n",
    "* Because $IoU(f_w(x), y)$ is itself a differentiable (or partially differentiable) function of $w$,\n",
    "\n",
    "* The gradient $\\frac{\\partial L_w}{\\partial w}$ will include two terms: one flowing through the confidence prediction $C(f_w(x))$ and another flowing through the IoU computation $IoU(f_w(x),y)$.\n",
    "\n",
    "Mathematically:\n",
    " $$\\frac{\\partial L_w}{\\partial w} = 2 \\left[ C(f_w(x)) - IoU(f_w(x), y) \\right] \\times \\left[ \\frac{\\partial}{\\partial w}  C(f_w(x)) - \\frac{\\partial}{\\partial w}  IoU(f_w(x), y) \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 2: IoU is computed extrenally and treated as a constant\n",
    "We have:\n",
    "$$L_w = \\left( C\\left(f_w(x)\\right) - IoU \\right)^2$$\n",
    "\n",
    "Where $IoU$ is a fixed number from PyTorch’s perspective, i.e. not connected to $f_w(x)$ in the computational graph.\n",
    "\n",
    "* Now, $IoU$ is effectively a constant with respect to $w$.\n",
    "\n",
    "* Thus $\\frac{\\partial}{\\partial w}IoU = 0$\n",
    "\n",
    "Mathematically:\n",
    " $$\\frac{\\partial L_w}{\\partial w} = 2 \\left[ C(f_w(x)) - IoU \\right] \\times \\left[ \\frac{\\partial}{\\partial w}  C(f_w(x))  \\right]$$\n",
    "\n",
    " There is no term $\\frac{\\partial}{\\partial w}IoU$ because we never computed the IoU from $\\widehat{x}, \\widehat{y}, \\widehat{w}, \\widehat{h}$ in a differentiable way. As a result, the bounding-box coordinates receive no gradient from the IoU-based confidence loss.\n",
    "\n",
    " In this case, you still get a gradient through $C(f_w(x))$, so the network can learn to adjust its confidence scores. However, you lose the pathway that would update the bounding-box coordinates $(\\widehat{x}, \\widehat{y}, \\widehat{w}, \\widehat{h})$ based on IoU. Consequently:\n",
    "* The coordinate parameters only get gradient from the direct coordinate regression loss"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
